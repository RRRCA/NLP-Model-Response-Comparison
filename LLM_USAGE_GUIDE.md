# LLM Response Generation Guide

è¿™æ˜¯Junyao Liuçš„ä»»åŠ¡æŒ‡å—ï¼šä½¿ç”¨GPT-3.5ç”Ÿæˆå¯¹è¯å“åº”å¹¶è®¡ç®—è¯„ä¼°æŒ‡æ ‡ã€‚

## ğŸ“‹ ä»»åŠ¡æ¸…å•

- [ ] å®‰è£…æ‰€éœ€ä¾èµ–
- [ ] è®¾ç½®OpenAI APIå¯†é’¥
- [ ] ç”ŸæˆLLMå“åº”ï¼ˆ20æ¡æµ‹è¯•æ ·æœ¬ï¼‰
- [ ] è®¡ç®—è‡ªåŠ¨è¯„ä¼°æŒ‡æ ‡ï¼ˆBERTScore, BLEU, Distinct-nï¼‰
- [ ] è®°å½•å»¶è¿Ÿå’Œtokenæˆæœ¬
- [ ] è¿›è¡Œå®šæ€§é”™è¯¯åˆ†æ

---

## ğŸ› ï¸ å®‰è£…ä¾èµ–

```bash
# å®‰è£…åŸºç¡€ä¾èµ–
pip install openai bert-score nltk numpy tqdm

# æˆ–è€…ä½¿ç”¨requirementsæ–‡ä»¶
pip install -r requirements_llm.txt
```

---

## ğŸ”‘ è®¾ç½®OpenAI APIå¯†é’¥

### æ–¹æ³•1ï¼šç¯å¢ƒå˜é‡ï¼ˆæ¨èï¼‰
```bash
export OPENAI_API_KEY='your-api-key-here'
```

### æ–¹æ³•2ï¼šå‘½ä»¤è¡Œå‚æ•°
```bash
python generate_llm_responses.py --api-key 'your-api-key-here'
```

---

## ğŸ“ ç”ŸæˆLLMå“åº”

### åŸºç¡€ä½¿ç”¨ï¼ˆç”Ÿæˆ20æ¡æ ·æœ¬ï¼‰

```bash
# å¤„ç†ä¸¤ä¸ªæ•°æ®é›†ï¼Œå„20æ¡æ ·æœ¬
python generate_llm_responses.py --num-samples 20

# åªå¤„ç†DailyDialog
python generate_llm_responses.py --dataset dailydialog --num-samples 20

# åªå¤„ç†EmpatheticDialogues
python generate_llm_responses.py --dataset empathetic_dialogues --num-samples 20
```

### è‡ªå®šä¹‰å‚æ•°

```bash
python generate_llm_responses.py \
  --dataset both \
  --num-samples 20 \
  --model gpt-3.5-turbo \
  --temperature 0.7 \
  --max-tokens 150 \
  --output-dir data/llm_outputs
```

### å‚æ•°è¯´æ˜

- `--dataset`: é€‰æ‹©æ•°æ®é›† (`dailydialog`, `empathetic_dialogues`, `both`)
- `--num-samples`: æ¯ä¸ªæ•°æ®é›†å¤„ç†çš„æ ·æœ¬æ•°é‡ï¼ˆé»˜è®¤20ï¼‰
- `--model`: OpenAIæ¨¡å‹åç§°ï¼ˆé»˜è®¤ `gpt-3.5-turbo`ï¼‰
- `--temperature`: é‡‡æ ·æ¸©åº¦ï¼ˆé»˜è®¤0.7ï¼‰
- `--max-tokens`: ç”Ÿæˆçš„æœ€å¤§tokenæ•°ï¼ˆé»˜è®¤150ï¼‰
- `--output-dir`: è¾“å‡ºç›®å½•ï¼ˆé»˜è®¤ `data/llm_outputs`ï¼‰

---

## ğŸ“Š è®¡ç®—è¯„ä¼°æŒ‡æ ‡

ç”Ÿæˆå“åº”åï¼Œè®¡ç®—è‡ªåŠ¨è¯„ä¼°æŒ‡æ ‡ï¼š

```bash
# è®¡ç®—DailyDialogçš„æŒ‡æ ‡
python compute_llm_metrics.py \
  --input data/llm_outputs/dailydialog_gpt35_responses.jsonl \
  --output data/llm_outputs/dailydialog_metrics.json

# è®¡ç®—EmpatheticDialoguesçš„æŒ‡æ ‡
python compute_llm_metrics.py \
  --input data/llm_outputs/empathetic_dialogues_gpt35_responses.jsonl \
  --output data/llm_outputs/empathetic_dialogues_metrics.json
```

### ä½¿ç”¨è‡ªå®šä¹‰BERTScoreæ¨¡å‹

```bash
python compute_llm_metrics.py \
  --input data/llm_outputs/dailydialog_gpt35_responses.jsonl \
  --output data/llm_outputs/dailydialog_metrics.json \
  --bertscore-model microsoft/deberta-xlarge-mnli
```

---

## ğŸ“‚ è¾“å‡ºæ–‡ä»¶ç»“æ„

è¿è¡Œè„šæœ¬åï¼Œä½ å°†å¾—åˆ°ï¼š

```
data/llm_outputs/
â”œâ”€â”€ dailydialog_gpt35_responses.jsonl          # ç”Ÿæˆçš„å“åº”
â”œâ”€â”€ dailydialog_gpt35_responses_summary.json   # ç”Ÿæˆæ‘˜è¦
â”œâ”€â”€ dailydialog_metrics.json                   # è¯„ä¼°æŒ‡æ ‡
â”œâ”€â”€ empathetic_dialogues_gpt35_responses.jsonl
â”œâ”€â”€ empathetic_dialogues_gpt35_responses_summary.json
â””â”€â”€ empathetic_dialogues_metrics.json
```

---

## ğŸ“„ è¾“å‡ºæ–‡ä»¶æ ¼å¼

### 1. å“åº”æ–‡ä»¶ (`*_responses.jsonl`)

æ¯ä¸€è¡Œæ˜¯ä¸€ä¸ªJSONå¯¹è±¡ï¼š

```json
{
  "dataset": "dailydialog",
  "dialog_id": "dd_test_123",
  "turn_id": 5,
  "context": "Hello <eot> How are you?",
  "ground_truth_response": "I'm fine, thank you!",
  "generated_response": "I'm doing well, thanks for asking!",
  "emotion": 1,
  "model": "gpt-3.5-turbo",
  "temperature": 0.7,
  "max_tokens": 150,
  "latency_seconds": 1.234,
  "token_usage": {
    "prompt_tokens": 45,
    "completion_tokens": 12,
    "total_tokens": 57
  }
}
```

### 2. æ‘˜è¦æ–‡ä»¶ (`*_summary.json`)

```json
{
  "dataset": "dailydialog",
  "model": "gpt-3.5-turbo",
  "num_samples": 20,
  "total_latency_seconds": 24.56,
  "total_tokens": 1140,
  "avg_latency_seconds": 1.228,
  "avg_tokens_per_sample": 57.0,
  "temperature": 0.7,
  "max_tokens": 150
}
```

### 3. æŒ‡æ ‡æ–‡ä»¶ (`*_metrics.json`)

```json
{
  "dataset": "dailydialog",
  "model": "gpt-3.5-turbo",
  "num_samples": 20,
  "bertscore_model": "microsoft/deberta-xlarge-mnli",
  "metrics": {
    "bleu-1": 0.2345,
    "bleu-2": 0.1567,
    "bleu-3": 0.0987,
    "bleu-4": 0.0543,
    "distinct-1": 0.6789,
    "distinct-2": 0.8234,
    "bertscore_precision": 0.8765,
    "bertscore_recall": 0.8543,
    "bertscore_f1": 0.8654,
    "avg_response_length": 15.3,
    "total_latency_seconds": 24.56,
    "avg_latency_seconds": 1.228,
    "total_tokens": 1140
  }
}
```

---

## ğŸ” å®šæ€§é”™è¯¯åˆ†æ

æŸ¥çœ‹ç”Ÿæˆçš„å“åº”æ–‡ä»¶ï¼Œåˆ†æï¼š

1. **è¯­ä¹‰å‡†ç¡®æ€§**ï¼šå“åº”æ˜¯å¦ä¸ä¸Šä¸‹æ–‡ç›¸å…³ï¼Ÿ
2. **æµç•…æ€§**ï¼šè¯­æ³•æ˜¯å¦æ­£ç¡®ï¼Œè¡¨è¾¾æ˜¯å¦è‡ªç„¶ï¼Ÿ
3. **æƒ…æ„Ÿä¸€è‡´æ€§**ï¼šæ˜¯å¦åŒ¹é…å¯¹è¯çš„æƒ…æ„Ÿï¼Ÿ
4. **å¤šæ ·æ€§**ï¼šå“åº”æ˜¯å¦è¿‡äºé‡å¤æˆ–æ¨¡æ¿åŒ–ï¼Ÿ

### ç¤ºä¾‹åˆ†æä»£ç 

```python
import json

# åŠ è½½å“åº”
with open('data/llm_outputs/dailydialog_gpt35_responses.jsonl', 'r') as f:
    responses = [json.loads(line) for line in f]

# æŸ¥çœ‹å‰å‡ ä¸ªæ ·æœ¬
for i, r in enumerate(responses[:5]):
    print(f"\n=== Sample {i+1} ===")
    print(f"Context: {r['context']}")
    print(f"Ground Truth: {r['ground_truth_response']}")
    print(f"Generated: {r['generated_response']}")
    print(f"Latency: {r['latency_seconds']:.3f}s")
    print(f"Tokens: {r['token_usage']['total_tokens']}")
```

---

## âš ï¸ æ³¨æ„äº‹é¡¹

1. **APIæˆæœ¬**ï¼šGPT-3.5-turboçº¦ $0.0015/1K tokens (prompt) + $0.002/1K tokens (completion)
   - 20æ¡æ ·æœ¬é¢„è®¡èŠ±è´¹çº¦ $0.05-0.10

2. **Rate Limits**ï¼šè„šæœ¬å·²æ·»åŠ 0.1ç§’å»¶è¿Ÿä»¥é¿å…é€Ÿç‡é™åˆ¶

3. **ç¯å¢ƒå˜é‡**ï¼šç¡®ä¿è®¾ç½®äº† `OPENAI_API_KEY`

4. **ä¾èµ–ç‰ˆæœ¬**ï¼š
   - openai >= 0.27.0
   - bert-score >= 0.3.13
   - nltk >= 3.8

---

## ğŸš€ å®Œæ•´å·¥ä½œæµç¨‹

```bash
# Step 1: è®¾ç½®APIå¯†é’¥
export OPENAI_API_KEY='your-api-key-here'

# Step 2: ç”Ÿæˆå“åº”ï¼ˆ20æ¡æ ·æœ¬ï¼‰
python generate_llm_responses.py --num-samples 20

# Step 3: è®¡ç®—DailyDialogæŒ‡æ ‡
python compute_llm_metrics.py \
  --input data/llm_outputs/dailydialog_gpt35_responses.jsonl \
  --output data/llm_outputs/dailydialog_metrics.json

# Step 4: è®¡ç®—EmpatheticDialoguesæŒ‡æ ‡
python compute_llm_metrics.py \
  --input data/llm_outputs/empathetic_dialogues_gpt35_responses.jsonl \
  --output data/llm_outputs/empathetic_dialogues_metrics.json

# Step 5: æŸ¥çœ‹ç»“æœ
cat data/llm_outputs/dailydialog_metrics.json
cat data/llm_outputs/empathetic_dialogues_metrics.json
```

---

## ğŸ“ é—®é¢˜æ’æŸ¥

### é—®é¢˜1ï¼šAPIå¯†é’¥é”™è¯¯
```
Error: Incorrect API key provided
```
**è§£å†³**ï¼šæ£€æŸ¥ `OPENAI_API_KEY` ç¯å¢ƒå˜é‡æ˜¯å¦è®¾ç½®æ­£ç¡®

### é—®é¢˜2ï¼šBERTScoreå®‰è£…å¤±è´¥
```
Warning: bert_score not installed
```
**è§£å†³**ï¼š
```bash
pip install bert-score
```

### é—®é¢˜3ï¼šNLTKæ•°æ®ç¼ºå¤±
```
LookupError: Resource punkt not found
```
**è§£å†³**ï¼š
```python
import nltk
nltk.download('punkt')
```

---

## ğŸ“ˆ ä¸‹ä¸€æ­¥

å®Œæˆç”Ÿæˆå’Œè¯„ä¼°åï¼š
1. âœ… å°†ç»“æœæ–‡ä»¶ä¿å­˜åˆ°é¡¹ç›®ç›®å½•
2. âœ… è®°å½•å…³é”®æŒ‡æ ‡ï¼ˆBLEU, BERTScore, latency, tokensï¼‰
3. âœ… è¿›è¡Œå®šæ€§åˆ†æï¼Œæ‰¾å‡ºä¼˜ç¼ºç‚¹
4. âœ… ä¸Hanxiao Wangçš„å°æ¨¡å‹ç»“æœè¿›è¡Œå¯¹æ¯”
5. âœ… å‡†å¤‡discussionå’Œpresentationææ–™

Good luck! ğŸ‰

